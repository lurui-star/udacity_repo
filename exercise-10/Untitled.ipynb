{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ab176a-a270-4914-af87-d7eab70bb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, FunctionTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50260a2f-7a9d-4e28-b7b5-2d3dfc181fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)-15s %(message)s\")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab9a8d1a-8e3f-4de9-9b2d-f0497f9083d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def go(args):\n",
    "\n",
    "    run = wandb.init(project=\"exercise_10\", job_type=\"train\")\n",
    "\n",
    "    logger.info(\"Downloading and reading train artifact\")\n",
    "    train_data_path = run.use_artifact(args.train_data).file()\n",
    "    df = pd.read_csv(train_data_path, low_memory=False)\n",
    "\n",
    "    # Extract the target from the features\n",
    "    logger.info(\"Extracting target from dataframe\")\n",
    "    X = df.copy()\n",
    "    y = X.pop(\"genre\")\n",
    "\n",
    "    logger.info(\"Splitting train/val\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    logger.info(\"Setting up pipeline\")\n",
    "\n",
    "    pipe = get_inference_pipeline(args)\n",
    "\n",
    "    logger.info(\"Fitting\")\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    logger.info(\"Scoring\")\n",
    "    score = roc_auc_score(\n",
    "        y_val, pipe.predict_proba(X_val), average=\"macro\", multi_class=\"ovo\"\n",
    "    )\n",
    "\n",
    "    run.summary[\"AUC\"] = score\n",
    "\n",
    "    # We collect the feature importance for all non-nlp features first\n",
    "    feat_names = np.array(\n",
    "        pipe[\"preprocessor\"].transformers[0][-1]\n",
    "        + pipe[\"preprocessor\"].transformers[1][-1]\n",
    "    )\n",
    "    feat_imp = pipe[\"classifier\"].feature_importances_[: len(feat_names)]\n",
    "\n",
    "    # For the NLP feature we sum across all the TF-IDF dimensions into a global\n",
    "    # NLP importance\n",
    "    nlp_importance = sum(pipe[\"classifier\"].feature_importances_[len(feat_names) :])\n",
    "\n",
    "    feat_imp = np.append(feat_imp, nlp_importance)\n",
    "    feat_names = np.append(feat_names, \"title + song_name\")\n",
    "\n",
    "    fig_feat_imp, sub_feat_imp = plt.subplots(figsize=(10, 10))\n",
    "    idx = np.argsort(feat_imp)[::-1]\n",
    "    sub_feat_imp.bar(range(feat_imp.shape[0]), feat_imp[idx], color=\"r\", align=\"center\")\n",
    "    _ = sub_feat_imp.set_xticks(range(feat_imp.shape[0]))\n",
    "    _ = sub_feat_imp.set_xticklabels(feat_names[idx], rotation=90)\n",
    "\n",
    "    fig_feat_imp.tight_layout()\n",
    "\n",
    "    fig_cm, sub_cm = plt.subplots(figsize=(10, 10))\n",
    "    plot_confusion_matrix(\n",
    "        pipe,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        ax=sub_cm,\n",
    "        normalize=\"true\",\n",
    "        values_format=\".1f\",\n",
    "        xticks_rotation=90,\n",
    "    )\n",
    "    fig_cm.tight_layout()\n",
    "\n",
    "    run.log(\n",
    "        {\n",
    "            \"feature_importance\": wandb.Image(fig_feat_imp),\n",
    "            \"confusion_matrix\": wandb.Image(fig_cm),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4420163b-565b-4293-aa3a-30359a50d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_pipeline(args):\n",
    "    # We need 3 separate preprocessing \"tracks\":\n",
    "    # - one for categorical features\n",
    "    # - one for numerical features\n",
    "    # - one for textual (\"nlp\") features\n",
    "    # Categorical preprocessing pipeline\n",
    "    categorical_features = sorted([\"time_signature\", \"key\"])\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=0), OrdinalEncoder()\n",
    "    )\n",
    "    # Numerical preprocessing pipeline\n",
    "    numeric_features = sorted([\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"loudness\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        \"valence\",\n",
    "        \"tempo\",\n",
    "        \"duration_ms\",\n",
    "    ])\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"), StandardScaler()\n",
    "    )\n",
    "    # Textual (\"nlp\") preprocessing pipeline\n",
    "    nlp_features = [\"text_feature\"]\n",
    "    # This trick is needed because SimpleImputer wants a 2d input, but\n",
    "    # TfidfVectorizer wants a 1d input. So we reshape in between the two steps\n",
    "    reshape_to_1d = FunctionTransformer(np.reshape, kw_args={\"newshape\": -1})\n",
    "    nlp_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=\"\"),\n",
    "        reshape_to_1d,\n",
    "        TfidfVectorizer(binary=True),\n",
    "    )\n",
    "    # Put the 3 tracks together into one pipeline using the ColumnTransformer\n",
    "    # This also drops the columns that we are not explicitly transforming\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "            (\"nlp1\", nlp_transformer, nlp_features),\n",
    "        ],\n",
    "        remainder=\"drop\",  # This drops the columns that we do not transform\n",
    "    )\n",
    "    # Get the configuration for the model\n",
    "    with open(args.model_config) as fp:\n",
    "        model_config = json.load(fp)\n",
    "    # Add it to the W&B configuration so the values for the hyperparams\n",
    "    # are tracked\n",
    "    wandb.config.update(model_config)\n",
    "    # Append classifier to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(**model_config)),\n",
    "        ]\n",
    "    )\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3af7741b-934f-4619-9f9e-089371b6078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --train_data TRAIN_DATA --model_config\n",
      "                             MODEL_CONFIG\n",
      "ipykernel_launcher.py: error: the following arguments are required: --train_data, --model_config\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruilu/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Train a Random Forest\",\n",
    "        fromfile_prefix_chars=\"@\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--train_data\",\n",
    "        type=str,\n",
    "        help=\"Fully-qualified name for the training data artifact\",\n",
    "        required=True,\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--model_config\",\n",
    "        type=str,\n",
    "        help=\"Path to a JSON file containing the configuration for the random forest\",\n",
    "        required=True,\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    go(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76b0e6-a9ee-43d9-8216-a5fa1bbe16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    " python run.py --train_data {train_data} \\\n",
    "                    --model_config {model_config}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
